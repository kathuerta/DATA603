{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based and Collaborative Filtering Recommender Systems\n",
    "Katherine Huerta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started (Overall)\n",
    "### Prerequisites\n",
    "* Python 3.7 (python >3.5= should work as well)\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* jupyter notebook\n",
    "\n",
    "#### Optional\n",
    "\n",
    "I used Anaconda as my platform. It provides Jupyter Notebook and allows you to install packages easily. Instructions on installation are provided below:\n",
    "\n",
    "[Anaconda Installation on Windows](https://docs.anaconda.com/anaconda/install/windows/)\n",
    "\n",
    "[Anaconda Installation on macOS](https://docs.anaconda.com/anaconda/install/mac-os/)\n",
    "\n",
    "[Anaconda Installation on Linux](https://docs.anaconda.com/anaconda/install/linux/)\n",
    "\n",
    "Additional information for getting started with Python and Anaconda can be found [here](https://docs.anaconda.com/anaconda/user-guide/getting-started/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Content-Based Filtering\n",
    "\n",
    "In the first notebook, we create two **content-based** movie recommenders. The first recommender is based on the plot description. The user can type in a movie they enjoyed, and the recommendations will be based on similarities to the plot description of the input movie.\n",
    "\n",
    "The second recommender is based on credits, genres, and keywords. The user will provide a movie as an input, and the resulting recommendations will be based on movies with similar genre, actors, director, and keywords. \n",
    "\n",
    "We compute similarity scores by first computing Term Frequency-Inverse Document Frequency (TF-IDF) vectors. The dot product of this matrix yields the cosine similarity score.\n",
    "\n",
    "We then define a function that takes a movie title as an input, where the output is a list of the ten most similar movies. \n",
    "\n",
    "Finally, we test it out to see what our recommendations are!\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "* scikit-learn\n",
    "    * sklearn:\n",
    "        * TfidfVectorizer\n",
    "        * CountVectorizer\n",
    "        * linear_kernel\n",
    "        * cosine_similarity\n",
    "* ast: literal_eval\n",
    "\n",
    "Ast should already be part of your environment. You can install scikit-learn following these [instructions](https://scikit-learn.org/stable/install.html). If you are using the anaconda navigator as suggested, simply paste this code into your anaconda prompt:\n",
    "```\n",
    "conda install scikit-learn\n",
    "```\n",
    "\n",
    "\n",
    "#### Code for importing packages\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from ast import literal_eval\n",
    "```\n",
    "#### Download the datasets, notebook, and image\n",
    "1. Download my folder (Huerta_Katherine_Project) and extract the folder\n",
    "2. From the folder **Part 1**, upload the following into your jupyter notebook environment\n",
    "    * upload tmdb_5000_credits.csv and tmdb_5000_movies.csv from the **tmdb-movie-metadata** folder.\n",
    "    * upload the content_based_image from the folder as well (if you want to view the image)\n",
    "    * upload the notebook file (Huerta_Part1_ContentBased.ipnyb)\n",
    "    * **Note** Keep the file names as they are, so the code works without modification. Make sure all files are in the same folder that the notebook is in.\n",
    "3. Read in the datasets using pandas. The code example below is already included in my notebook, so no need to do this separately\n",
    "```python\n",
    "df1 = pd.read_csv('tmdb_5000_credits.csv')\n",
    "df2 = pd.read_csv('tmdb_5000_movies.csv')\n",
    "```\n",
    "\n",
    "## Datasets\n",
    "* Using the [TMDB 5000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata) from Kaggle.\n",
    "    * This dataset contains metadata on approximately 5,000 movies from TMDb. This makes it an ideal dataset for a content-based recommendation system.\n",
    "   1. tmdb_5000_credits.csv\n",
    "   2. tmdb_5000_movies.csv\n",
    "    * Both datasets are located in the tmdb-movie-metadata folder (\\Huerta_Katherine_Project\\Part 1)\n",
    "    * More information on these datasets can be found in my jupyter notebook file (Huerta_Part1_ContentBased), or using the link above. \n",
    "\n",
    "# Acknowledgments\n",
    "1. [TMDB 5000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata) Metadata on ~5,000 movies from TMDb\n",
    "2. [Ibtesam Ahmed](https://www.kaggle.com/ibtesama/getting-started-with-a-movie-recommendation-system) Getting Started with a Movie Recommendation System\n",
    "3. Image link for introduction: [Emma Grimaldi](https://towardsdatascience.com/how-to-build-from-scratch-a-content-based-movie-recommender-with-natural-language-processing-25ad400eb243)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Collaborative Filtering Recommender Systems\n",
    "\n",
    "The idea behind this recommendation engine is to attempt to predict the rating that a user would give a movie. The dataset we are using is a list of movies, users, and user ratings with links to tmdb (theMovieDB) web IDs. This allows us to obtain information from APIs related to our recommendations.\n",
    "\n",
    "Essentially, we will build a matrix of users X movies with their rating values filled in for existing data. This feature set is decomposed into UxV matrices that relate to a set of latent factors that are used to describe user preferences, and ultimately their expected ratings.\n",
    "\n",
    "Apache Spark ML uses alternating least squares (ALS) for collaborative filtering, and it is a common algorithm used for recommenders. An ALS recommender is a matrix factorization algorithm that uses ALS with Weighted-Lambda-Regularization (ALS-WR). This reduces the dependency of the regularization paramater on the scale of the dataset. Thus, the best parameter learned from a sample subset is applicable to the entire dataset with similar performance. The latent factors explain the user of interest's movie ratings and map new users to optimized movie recommendations. \n",
    "\n",
    "## Getting Started\n",
    "Most of these packages' imports are included in the notebook, but some are not immediately available and will need more instruction/preparation before running the code. If you already have Apache Spark and pyspark, you should be fine to run **most** of the code. **However, see tmdbsimple bullet under prerequesites to ensure you can display the movie poster visualization** \n",
    "### Prerequisites\n",
    "* math\n",
    "* os, sys, requests, json\n",
    "* time\n",
    "* Image\n",
    "* display\n",
    "* Apache Spark\n",
    "    1. Must have [Java 8](https://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html) or higher installed\n",
    "    2. Go to [Spark downloads page](http://spark.apache.org/downloads.html), select latest Spark release (prebuilt package for Hadoop), download it directly, and unzip and move it to the appropriate folder (video on how to set up spark can be found [here](https://www.youtube.com/watch?v=VYNsaR-gOsA), and another helpful article that includes pyspark implementation can be found [here](https://www.sicara.ai/blog/2017-05-02-get-started-pyspark-jupyter-notebook-3-minutes) - highly suggest you watch if you don't have it installed already).\n",
    "\n",
    "* pyspark\n",
    "    1. Once you have Spark set up, there are many ways to configure pyspark with jupyter notebook: \n",
    "        * Anaconda has its own pyspark package. You can simply search for pyspark in your anaconda environment (make sure you check the \"not installed\" option first) and install it.\n",
    "        * More installation options can be found in [Anaconda Cloud Packages](https://anaconda.org/conda-forge/findspark)\n",
    "        * You can install conda findspark to access spark instance from jupyter notebook, enabling pyspark to be importable. \n",
    "            * Conda prompt:\n",
    "            ``` \n",
    "            conda install -c conda-forge findspark\n",
    "            ```\n",
    "            * Then open Jupyter notebook and input the following code:\n",
    "            ```\n",
    "            import findspark\n",
    "            findspark.init()\n",
    "            findspark.find()\n",
    "            import pyspark\n",
    "            ```\n",
    "        * You can explore the pyspark package API in Apache Spark's [PySpark 2.4.4 documentation](https://spark.apache.org/docs/latest/api/python/pyspark.html)\n",
    "    2. Packages from pyspark used in this notebook:\n",
    "        * functions\n",
    "        * DataFrameNaFunctions\n",
    "        * udf, col, when\n",
    "        * RegressionEvaluator\n",
    "        * ALS\n",
    "        * CrossValidator\n",
    "        * ParamGridBuilder\n",
    "        * Pipeline\n",
    "        * Row\n",
    "        * SQLContext\n",
    "       \n",
    "* **tmdbsimple** (for displaying movie posters)\n",
    "    1. Make an account on [themoviedb.org](https://www.themoviedb.org/). Instructions can be found [here](https://developers.themoviedb.org/3/getting-started/introduction).\n",
    "        * **Note:** Please use your own API key. \n",
    "    2. To install tmdbsimple:\n",
    "        * If you are using the anaconda platform, you can add it to your environment following instructions provided by [Anaconda Cloud](https://anaconda.org/anjos/tmdbsimple). Simply use the anaconda (not python) prompt to input this code for installation of this package:\n",
    "        ```\n",
    "        conda install -c anjos tmdbsimple\n",
    "        ```\n",
    "        * Instructions on how to install using pip can be found [here](https://pypi.org/project/tmdbsimple/)\n",
    "        \n",
    "#### Example Code for Prerequisites:\n",
    "This is already included in the notebook, but just in case you wanted to know which pyspark subpackages are being used, or how the tmdb API key will be used, see the following:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrameNaFunctions as DFna\n",
    "from pyspark.sql.functions import udf, col, when\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark as ps\n",
    "import os, sys, requests, json\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "import time\n",
    "import requests\n",
    "\n",
    "import tmdbsimple as tmdb\n",
    "tmdb.api_key = 'Your_API_Key'\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "```\n",
    "\n",
    "#### Download the datasets, notebook, and image\n",
    "\n",
    "1. Download my folder (Huerta_Katherine_Project) and extract/unzip the folder\n",
    "2. From the folder **Part 2**, upload the following into your jupyter notebook environment\n",
    "    * upload files from the **ml-latest-small** folder to jupyter notebook.\n",
    "    * upload the two images from the images folder as well\n",
    "    * upload the notebook file (Huerta_Part2_CollaborativeFiltering_SparkALS.ipnyb)\n",
    "    * **Note** Keep the file names as they are, so the code works without modification. Make sure all files are in the same folder that the notebook is in.\n",
    "3. Read in the datasets using `spark.read.csv`. The code example below is already included in my notebook. Doing this prematurely will result in failure because you need to start a SparkSession and create a Spark Context.\n",
    "```python\n",
    "# read movies CSV\n",
    "movies_df = spark.read.csv('movies.csv',\n",
    "                         header=True,       # use headers\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # Infer Schema\n",
    "movies_df.printSchema()\n",
    "```\n",
    "\n",
    "```python\n",
    "# read ratings CSV\n",
    "ratings_df = spark.read.csv('ratings.csv',\n",
    "                         header=True,       # use headers\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # infer schema\n",
    "ratings_df.printSchema()\n",
    "```\n",
    "\n",
    "```python\n",
    "# read links CSV\n",
    "links_df = spark.read.csv('links.csv',\n",
    "                         header=True,       # use headers\n",
    "                         quote='\"',         # char for quotes\n",
    "                         sep=\",\",           # char for separation\n",
    "                         inferSchema=True)  # infer schema\n",
    "links_df.printSchema()\n",
    "```\n",
    "\n",
    "\n",
    "## Datasets\n",
    "* Using the MovieLens dataset provided by [GroupLens](https://grouplens.org/datasets/movielens/)\n",
    "* Specifically, using [MovieLens Latest Datasets](https://grouplens.org/datasets/movielens/latest/)\n",
    "    * For simulation, use **ml-latest-small.zip** (size: 1 MB)\n",
    "        * _Small_: ~100,000 movie ratings from users (on a 1-5 scale) and ~3,600 tag applications applied to ~9,000 movies by ~600 users (last updated in 2018).\n",
    "    * Also works for **ml-latest.zip**, but is to large to submit on blackboard (size: 265 MB).\n",
    "        * _Full_: 27,000,000 ratings and 1,100,000 tag applications applied to 58,000 movies by 280,000 users (2018).\n",
    "* The datasets can be found in the **ml-latest-small** folder (\\Huerta_Katherine_Project\\Part 2) and contains:\n",
    "    * `ratings.csv` \n",
    "        * All ratings contained in this file\n",
    "        * Each line of this file after the header row represents one rating of one movie by one user\n",
    "        * File format:\n",
    "        ``` userId,movieId,rating,timestamp ```\n",
    "    * `tags.csv`\n",
    "        * All tags contained in this file\n",
    "        * Each line of this file after the header row represents one tag applied to one movie by one user\n",
    "        * File format:\n",
    "        ```userId,movieId,tag,timestamp```\n",
    "    * `movies.csv`\n",
    "        * Movies data file\n",
    "        * Contains movie information\n",
    "        * Each line of file after header represents one movie\n",
    "        * File format:\n",
    "        ```movieId,title,genres```\n",
    "    * `links.csv`\n",
    "        * Links data file\n",
    "        * Contains identifiers that can be used to link to other sources of movie data.\n",
    "        * Each line of file after header row represents one movie\n",
    "        * File format:\n",
    "        ```movieId,imdbId,tmdbId```\n",
    "        * **used for displaying poster images**\n",
    "            * movieId - identifier for movies used by <https://movielens.org>\n",
    "                * Ex: Movie \"Toy Story\" has link <https://movielens.org/movies/1> \n",
    "            * imdbId - identifier for movies used by <http://www.imdb.com>\n",
    "            * **tmdbId** - identifier for movies used by <https://www.themoviedb.org>.\n",
    "                * Toy Story has the link <https://www.themoviedb.org/movie/862>.\n",
    "                * **identifier used in this system**\n",
    "\n",
    "\n",
    "\n",
    "### About:\n",
    "* **userId**: \n",
    "    * User Ids (integer)\n",
    "    * randomly selected users. User ids are consistent between `ratings.csv` and `tags.csv` (i.e., the same id refers to the same user across the two files).\n",
    "* **movieId**:\n",
    "    * Movie Ids\n",
    "    * Only movies with at least one rating or tag are included in the dataset. These movie ids are consistent with those used on the MovieLens web site (id `1` corresponds to the URL <https://movielens.org/movies/1>). Movie ids are consistent between `ratings.csv`, `tags.csv`, `movies.csv`, and `links.csv` (the same id refers to the same movie across these four data files).\n",
    "* **rating**:\n",
    "    * User ratings\n",
    "    * Ratings are on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "* **timestamp**: seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970.\n",
    "\n",
    "* **tag**:\n",
    "    * user-generated metadata about movies. \n",
    "    * Each tag is usually a single word or short phrase. The meaning, value, and purpose of a particular tag is determined by each user.\n",
    "* **title**:\n",
    "    * Movie titles entered manually or imported from <https://www.themoviedb.org/>, and include the year of release in parentheses.\n",
    "* **genres**:Pipe-separated list. Selected from:\n",
    "\n",
    "    * Action\n",
    "    * Adventure\n",
    "    * Animation\n",
    "    * Children's\n",
    "    * Comedy\n",
    "    * Crime\n",
    "    * Documentary\n",
    "    * Drama\n",
    "    * Fantasy\n",
    "    * Film-Noir\n",
    "    * Horror\n",
    "    * Musical\n",
    "    * Mystery\n",
    "    * Romance\n",
    "    * Sci-Fi\n",
    "    * Thriller\n",
    "    * War\n",
    "    * Western\n",
    "    * (no genres listed)\n",
    "\n",
    "## Acknowledgments\n",
    "1. [GdMacmillan - spark_recommender_systems](https://github.com/GdMacmillan/spark_recommender_systems/blob/master/spark_recommender.ipynb) backbone of my code, however I had to make many modification for it to work for my environment/dataset\n",
    "2. [Apache Spark - Collaborative Filtering](https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html) \n",
    "3. [tmdbsimple 2.2.0](https://pypi.org/project/tmdbsimple/) _A Python wrapper for The Movie Database API v3_\n",
    "4. [The Movie DB](https://www.themoviedb.org/) for displaying pictures of movies. \n",
    "5. [GroupLens](https://grouplens.org/datasets/movielens/) [MovieLens](https://movielens.org/) dataset source. I used the ml-latest-small (most recent dataset containing 100,000 ratings, 3,600 tag applications applied to 9,000 movies by 600 users). I used the small dataset to provide a faster simulation (size: 1 MB).\n",
    "6. [Mark Litwintschik](https://tech.marksblogg.com/recommendation-engine-spark-python.html) _Recommendation Engine built using Spark and Python_ Explains how a user can get predictions based on their own ratings of movies.\n",
    "7. [Abhinav Ajitsaria](https://realpython.com/build-recommendation-engine-collaborative-filtering/) Rating Matrix image used in introduction.\n",
    "8. [Kevin Liao](https://towardsdatascience.com/prototyping-a-recommender-system-step-by-step-part-2-alternating-least-square-als-matrix-4a76c58714a1) Prototyping a Recommender System Step by Step Part 2: Alternating Least Square (ALS) Matrix Factorization in Collaborative Filtering. _Used this article to learn more about ALS and Matrix Factorization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
